{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expanded literature review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Material and methods\n",
    "#### 3.1 Data Description\n",
    "The log data provided by the insurance company has 2 components, the message and the time stamp. Each piece of data has an activity type, including but not limited to: \"Quote Started\", \"Payment Completed\", \"Claim Started\", \"Claim Accepted\" and \"Claim Denied\". In reality a customer's claim could be denied for many reasons, but in this dataset \"Claim Denied\" are precisely the cases that are identified as frauds. \n",
    "\n",
    "The data is in the log form and therefore should go through data cleaning, data transformation and data curation processes to be in tabular form, on which we build a classifer of fraudulant activities. Challenges encountered with this data include:\n",
    "\n",
    "1. Not all data are relevant. We split the original data according to the activity type. \"Quote Completed\", \"Claim Started\", \"Claim Accepted\" and \"Claim Denied\" are the most important ones and are examined individually. \n",
    "\n",
    "\n",
    "2. There are several different delimer seperating fields in each piece of data. For example, the case ID, platform and activity type are separated by \"-\", whereas the time stamp and the rest of the information is separated by \",\".\n",
    "\n",
    "\n",
    "3. Due to confidentiality concerns, the insurance company's clients' information are transfromed and substituted by pseudo names. No meaningful conclusion can be drawn from these names and addresses directly, but the distribution of these data are kept intact. \n",
    "\n",
    "\n",
    "4. One customer ID could correspond to different payload, in which they submit different information about their households and homes. One customer ID could also submit multiple claims and there is no given link between a \"Claim Started\" and \"Claim Denied\" or \"Claim Accepted\" cases. We will process the data on the assumption that each \"Claim Started\" case corresponds to the most recent submitted payload from \"Quote Completed\". The customers with more than one payload and those who submitted more than one claims only constitute a small proportion of all data.\n",
    "\n",
    "We split the dataset by activity types and label each incoming case. The incoming cases are sorted according to their time stamps. Each incoming case has features attached to it, which are used to predict the label. Other than \"platform\" (the platform the claimer used), the majority of the features are extracted from the \"Quote Completed\" data, where the customers submitted Json payload describing their demographic and household information. \n",
    "\n",
    "We extracted an important feature not in the original data, which is \"matched_time_stamp\". From our observations, many claims were started right after the customer made the payment, mostly the fraudulent ones. The time stamps are exactly the same. \"matched_time_stamp\" simply checks if there is a \"Payment Completed\" activity associated with the same customer ID, at the exact same time a \"Claim Started\" activity happened. Our analysis validates that all of the frauds have this feature equalling \"True\", but not the other way around. There are still some cases with matched time stamps but are not labelled as \"Fraud\". \n",
    "\n",
    "\n",
    "#### 3.2 Assumptions\n",
    "(all the assumptions we made should be validated, or explained)\n",
    "1. One customer ID can onl represent one customer in real life. \n",
    "\n",
    "In the dataset, one customer ID could correspond to a lot of activities. One customer ID could even have different Json payloads (with claimer's name, age, gender and other basic information). In reality, most companies would not reuse its customer IDs as this creates unnecessary anomolies and inconsistencies. Therefore, in our analysis, each customer ID is treated as an individual customer. \n",
    "\n",
    "2. For the majority of the customers, their identities do not change over time. \n",
    "\n",
    "We assume that most normal customers would never start any fraudulent activities, at least not under the same customer ID. We also assume that most frauds would not turn from their wrongdoings and start making legitimate claims, at least not under the same ID. This assumption can be validated through the following analysis:\n",
    "\n",
    "As shown in the pie chart below, there exist only a few customers whose claims got denied and accepted at different times. These people only make up a small proportion of the population. For the rest of the customers, their claims either always got denied or always got accepted. \n",
    "\n",
    "// there should be a pie chart here \n",
    "\n",
    "Based on assumption 2, it is advisable to take a customer's claim history into account. If an existing customer makes a new claim, the claim history tells us a lot about whether the incoming claim is fraud or not. We define an additional feature for each incoming claim case, \"number of denials\". For each claim case along with a time stamp, this feature shows how many times a customer's claims got denied by that time stamp. For example, if a person got denied 5 times (although this is unlikely in the dataset), the \"number of denials\" for the 5 incoming cases should be 0,1,2,3,4, indicating how many times the person were denied already. \n",
    "\n",
    "#### 3.3 Methods\n",
    "Feature engineering is necessary to decide which predictors are most relevant. With the apporpriate features, we can build different classification models and compare their performances. Since the dataset is time-stamped, cross-validation is not appropriate to evaluate the performance of a classifier. It is unadvisable to use data from the future to make predictions in the past, as there might be information leak. We split the data using different split ratios. We train the model using data before a certain timeStamp and test the model on the data after that timeStamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Analyses and results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that the customers in this dataset can start multiple quotes and claims, which makes our analysis more complicated. Fortunately, only a smally portion of the customers who started multiple quotes were both denied and accepted at some point. The rest of the population either gets denied all the time or accepted all the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature that really separates the frauds from the norm is whether the timeStamp of activity \"Payment Completed\" and \"Claim Started\" are the same. Interestingly, all of the claims labelled fraud in this dataset has this property, but not the other way around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ P( fraud | matching \\ timeStamp ) = \\frac{ P(fraud \\ \\& \\ matching \\ timeStamp) }{P(matching \\ timeStamp)} = 10494/12377 = 0.8479 $$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex \n",
    "$$ P( fraud | matching \\ timeStamp ) = \\frac{ P(fraud \\ \\& \\ matching \\ timeStamp) }{P(matching \\ timeStamp)} = 10494/12377 = 0.8479 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. how many claims, how many accepted, how many denied \n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
